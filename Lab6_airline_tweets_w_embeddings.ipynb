{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlassner/DSML_4220_Deep_Learning/blob/main/Lab6_airline_tweets_w_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGa4QJ7ly2fC"
      },
      "source": [
        "# Lab 6: Airline Tweets with (and without) Embeddings\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/DSML4220/blob/main/lab6_airline_tweets_w_embeddings.ipynb)\n",
        "\n",
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/DSML4220/blob/main/lab6_airline_tweets_w_embeddings.ipynb)\n",
        "\n",
        "In this notebook we'll build revisit the Airline Tweets dataset (from [Lab 1](https://github.com/sgeinitz/DSML4220/blob/main/lab1_text_data.ipynb)) and compare using an MLP with one-hot encodings as the input vs using word embeddings as the input.\n",
        "\n",
        "In this lab there are three (3) questions/tasks. These questions are listed here but are also inline below.\n",
        "\n",
        "1. Q1: Choose two words to compare (different from \"_wonderful_\" vs \"_incredible_\"). Re-run the parts of the notebook that plot the histogram of the differences between learned weight parameter values for each of your chosen words across the 128 hidden units in the first layer.\n",
        "2. Q2: Add your two words the list of words whose embeddings are displayed and compared. Do your two chosen words have similar embeddings? In other words, is the distance between your embeddings very small?\n",
        "3. Q3: Compare the size of the two models used in this notebook, one of which uses one-hot encoded inputs and the other which uses GloVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqtzfvCgy2fF"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y numpy torch torchvision torchtext torchmetrics torchaudio transformers\n",
        "!pip install numpy==1.25.2\n",
        "!pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 torchtext==0.16.1 torchmetrics==0.11.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du74wDHty2fG"
      },
      "outputs": [],
      "source": [
        "#!pip install torchmetrics\n",
        "#!pip install torchmetrics tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDjoThC1y2fH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "#import tqdm #import notebook\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from torchmetrics.functional import pairwise_cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SGmmlk9y2fI"
      },
      "outputs": [],
      "source": [
        "data_URL = 'https://raw.githubusercontent.com/sgeinitz/DSML4220/main/data/airlinetweets.csv'\n",
        "df = pd.read_csv(data_URL)\n",
        "print(f\"df.shape: {df.shape}\")\n",
        "pd.set_option(\"display.max_colwidth\", 240)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh9JMmYdy2fJ"
      },
      "outputs": [],
      "source": [
        "random.seed(2)\n",
        "indices = list(range(len(df)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "df_test = df.iloc[indices[9000:],]\n",
        "df = df.iloc[indices[:9000],]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz3KqjXky2fJ"
      },
      "outputs": [],
      "source": [
        "df_test.shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__3TM1aAy2fJ"
      },
      "source": [
        "Recall that about 2/3 of the data have negative labels, and that the remaining labels are roughly split between positive and neutral (slightly more neutral than positive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t78bZ4e5y2fJ"
      },
      "outputs": [],
      "source": [
        "df.sentiment.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kRZt3PDy2fK"
      },
      "source": [
        "Let's start with the nltk TweetTokenizer, which will split the text into separate words and characters based on common Twitter conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo1IajIyy2fL"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tk = TweetTokenizer()\n",
        "df['tokens_raw'] = df['text'].apply(lambda x: tk.tokenize(x.lower()))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gdt4ywQy2fM"
      },
      "source": [
        "Previously, we did not do a lot of exploratory data analysis (EDA) on this airline tweet dataset. We will not do too much here either, but at the very least let's look at a histogram of the lengths of the tweets. Note that here we are defining length to be the number of tokens, but it may also be useful to look at the number of characters. And, of course, there are other EDA steps we could do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSHkWRDEy2fM"
      },
      "outputs": [],
      "source": [
        "df['tweet_length'] = df['tokens_raw'].apply(lambda x: len(x))\n",
        "plt.figure(figsize=(12,6))\n",
        "df['tweet_length'].hist() #bins=100, range=(0,45), width=0.9) #, df['tweet_length'].mean(), df['tweet_length'].median()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1Ci5_fby2fM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAlff-Eny2fN"
      },
      "source": [
        "Next, let's remove common stop words (e.g. \"_the_\", \"_in_\", etc.). In this next cell we will also remove some characters/punctuation, as well as hashtag tokens.\n",
        "\n",
        "Note: If the following cell causes an error, then uncomment the code cell above and run it to download and load the nltk stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxTNi8_cy2fN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "chars2remove = set(['.','!','/', '?'])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in stops])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if w not in chars2remove])\n",
        "df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^#', w)]) # remove hashtags\n",
        "#df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^http', w)]) # remove web links\n",
        "#df['tokens_raw'] = df['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^@', w)]) # remove web links\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbl_SU3Jy2fN"
      },
      "source": [
        "For the final step of text pre-processing we will lemmatize the tokens. Note that there are much better ways to do this but that we want to use a simple lemmatizer. For example, some lemmatizers also utilize a model internally to predict the part-of-speech for each word, since whether the word is a noun, adjective, verb, etc. will affect how lemmatization is done. Since we want to keep things simple here, and focus only on the lemmatization step, we'll assume every word is the same part of speech. Note that this is not by any means ideal (try to identify the incorrectly lemmatized token in the five tweets printed out below). In practice we would certainly utilize a 'smarter' lemmatizer.\n",
        "\n",
        "The last step below is to combined the tokens back into a single string, which is stored in the column `textclean`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OWTEniBy2fN"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
        "#df['tokens'] = df['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
        "\n",
        "df['textclean'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fhMoS8sy2fO"
      },
      "source": [
        "Now we will perform one-hot encoding using sklearn's, `CountVectorizer`, with the option `binary=True`. We'll go ahead and call the resulting vectorized data, `X`, or `X_train` since it is only the training dataset. As with conventional statistical models, \"_X_\" represents the set of predictors, or independent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WueM-PH1y2fO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#count_vectorizer = CountVectorizer(binary=True)\n",
        "count_vectorizer = CountVectorizer(binary=True, min_df=2)\n",
        "X_np = count_vectorizer.fit_transform(df['textclean']).toarray()\n",
        "\n",
        "print(f\"X_np.shape = {X_np.shape}\")\n",
        "type(X_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MX4I8HWy2fO"
      },
      "source": [
        "Here is the full vocabulary created by the the `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPkxLlSly2fO"
      },
      "outputs": [],
      "source": [
        "vocab = count_vectorizer.vocabulary_\n",
        "vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1], reverse=False)}\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un1MsavCy2fO"
      },
      "source": [
        "---\n",
        "\n",
        "### Q1: Choose two words to compare (different from \"_wonderful_\" vs \"_incredible_\").\n",
        "\n",
        "Below you will choose your two words, which have similar meaning and which you suspect the model will treat similarly. Then, re-train the model and plot the histogram of the differences between learned weight values for each of your chosen words across the 128 hidden units in the first layer. Did the histograms show that the learned weight values were similar for your words? More similar than for the neighboring words compared to each other?\n",
        "\n",
        "They were more similar to each other than neighboring words because in this context there is a good, neutral and bad label for all the tweets, so similar words in the \"good\" tweets can be grouped easily even without knowing the definition.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUVk4cWAy2fO"
      },
      "outputs": [],
      "source": [
        "word1 = 'plane'\n",
        "word2 = 'flight'\n",
        "\n",
        "word1_idx = vocab[word1]\n",
        "print(f\"The index for '{word1}': {word1_idx}\")\n",
        "\n",
        "word2_idx = vocab[word2]\n",
        "print(f\"The index for '{word2}': {word2_idx}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw1RWodNy2fP"
      },
      "source": [
        "Next, let's look at the tweets themselves that contained the word _\"great\"_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbHnypHJy2fP"
      },
      "outputs": [],
      "source": [
        "rows_w_word1 = np.where(X_np[:, word1_idx] == 1)[0]\n",
        "print(rows_w_word1)\n",
        "df.iloc[rows_w_word1,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp4oQiEgy2fP"
      },
      "outputs": [],
      "source": [
        "rows_w_word2 = np.where(X_np[:, word2_idx] == 1)[0]\n",
        "print(rows_w_word2)\n",
        "df.iloc[rows_w_word2,]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icoOfgRuy2fP"
      },
      "source": [
        "Confirm that the input, `X`, has n rows and a column for each word (token) in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrTt2RQMy2fP"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor(X_np, dtype=torch.float32)\n",
        "X.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nznLIRgpy2fP"
      },
      "outputs": [],
      "source": [
        "# look at one observation and see how may tokens there are (i.e. how many 1's are in the row, and how many 0's)\n",
        "pd.DataFrame(X_np[1,:]).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U80fJzb1y2fP"
      },
      "outputs": [],
      "source": [
        "labels = df['sentiment'].unique()\n",
        "enum_labels = enumerate(labels)\n",
        "label_to_idx = dict((lab, i) for i,lab in enum_labels)\n",
        "print(f\"label dictionary: {label_to_idx}\")\n",
        "y = torch.tensor([label_to_idx[lab] for lab in df['sentiment']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb_qfGzDy2fP"
      },
      "outputs": [],
      "source": [
        "class AirlineTweetDataset(Dataset):\n",
        "    def __init__(self, observations, labels):\n",
        "        self.obs = observations\n",
        "        self.labs = labels\n",
        "        self.create_split(len(observations))\n",
        "\n",
        "    def create_split(self, n, seed=2, train_perc=0.7):\n",
        "        random.seed(seed)\n",
        "        indices = list(range(n))\n",
        "        random.shuffle(indices)\n",
        "        self._train_ids = list(indices[:int(n * train_perc)])\n",
        "        self._test_ids = list(indices[int(n * train_perc):])\n",
        "        self._split_X = self.obs[self._train_ids]\n",
        "        self._split_y = self.labs[self._train_ids]\n",
        "\n",
        "    def set_split(self, split='train'):\n",
        "        if split == 'train':\n",
        "            self._split_X = self.obs[self._train_ids]\n",
        "            self._split_y = self.labs[self._train_ids]\n",
        "        else:\n",
        "            self._split_X = self.obs[self._test_ids]\n",
        "            self._split_y = self.labs[self._test_ids]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._split_y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'x':self._split_X[idx], 'y':self._split_y[idx]}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "dataset = AirlineTweetDataset(X, y)\n",
        "dataset.create_split(len(X), seed=42, train_perc=0.85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by0dJsz3y2fQ"
      },
      "outputs": [],
      "source": [
        "dataset.set_split('train')\n",
        "print(f\"len(dataset) = {len(dataset)}\")\n",
        "#len(dataset[:]['x'])\n",
        "dataset[0]['x']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEx5TLoky2fQ"
      },
      "source": [
        "Confirm that there are no NaN, and that all numerical values are finite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld_tYTnvy2fQ"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==2.0.2\n",
        "dataset[:]['x'].numpy()[0,:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTw24g_Py2fQ"
      },
      "outputs": [],
      "source": [
        "assert not np.any(np.isnan(dataset[:]['x'].numpy()))\n",
        "assert np.all(np.isfinite(dataset[:]['x'].numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avJKhdXty2fQ"
      },
      "outputs": [],
      "source": [
        "class AirlineTweetClassifier(nn.Module):\n",
        "    \"\"\" A 2-layer Multilayer Perceptron for classifying surnames \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input embeddings\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(AirlineTweetClassifier, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 32)\n",
        "        self.fc3 = nn.Linear(32, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        intermediate_vector = F.relu(self.fc1(x_in))\n",
        "\n",
        "        intermediate_vector = F.relu(self.fc2(intermediate_vector))\n",
        "        intermediate_vector = self.dropout(intermediate_vector)\n",
        "\n",
        "        prediction_vector = self.fc3(intermediate_vector)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMRzlRoYy2fQ"
      },
      "source": [
        "#### Hyperparameters for model with one-hot encoded inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngDocJK5y2fR"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vrsXsi_y2fR"
      },
      "source": [
        "Take one quick look at the size of the training and validation splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irP929uQy2fR"
      },
      "outputs": [],
      "source": [
        "dataset.set_split('train')\n",
        "#print(len(dataloader) * batch_size)\n",
        "dataset.set_split('val')\n",
        "#print(len(dataloader) * batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kQI5Odiy2fR"
      },
      "outputs": [],
      "source": [
        "seed = 2\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# create the dataset, model and define loss function and optimizer\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
        "model = AirlineTweetClassifier(len(dataset[0]['x']), 128, 3)\n",
        "loss_fun = nn.CrossEntropyLoss()#weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0stzAkHy2fR"
      },
      "outputs": [],
      "source": [
        "import tqdm.notebook\n",
        "epoch_bar = tqdm.notebook.tqdm(desc='training routine', total=num_epochs, position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm.notebook.tqdm(desc='split=train', total=dataset.get_num_batches(batch_size), position=1, leave=True)\n",
        "\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm.notebook.tqdm(desc='split=val', total=dataset.get_num_batches(batch_size), position=1, leave=True)\n",
        "\n",
        "losses = {'train':[], 'val':[]}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    model.train()\n",
        "    running_loss_train = 0.0\n",
        "\n",
        "    for batch_i, batch_data in enumerate(dataloader):\n",
        "        tweets = batch_data['x'].to(device)\n",
        "        labels = batch_data['y'].to(device)\n",
        "\n",
        "        # forward\n",
        "        outputs = model(tweets)\n",
        "        loss = loss_fun(outputs, labels)\n",
        "        losses['train'].append(loss.item())\n",
        "        running_loss_train += loss.item()\n",
        "\n",
        "        # backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #if (batch_i+1) % 10 == 0:\n",
        "        #    print(f\"    train batch {batch_i+1:3.0f} (of {len(dataloader):3.0f}) loss: {loss.item():.4f}\")\n",
        "            # update bar\n",
        "        train_bar.set_postfix(loss=running_loss_train, epoch=epoch)\n",
        "        train_bar.update()\n",
        "\n",
        "    train_bar.set_postfix(loss=running_loss_train/dataset.get_num_batches(batch_size), epoch=epoch)\n",
        "    train_bar.update()\n",
        "\n",
        "\n",
        "    running_loss_train = running_loss_train / len(dataset)\n",
        "\n",
        "    dataset.set_split('val')\n",
        "    model.eval() # turn off the automatic differentiation\n",
        "    running_loss_val = 0.0\n",
        "\n",
        "    for batch_i, batch_data in enumerate(dataloader):\n",
        "        tweets = batch_data['x'].to(device)\n",
        "        labels = batch_data['y'].to(device)\n",
        "\n",
        "\n",
        "        # forward (no backward step for validation data)\n",
        "        outputs = model(tweets)\n",
        "        loss = loss_fun(outputs, labels)\n",
        "        losses['val'].append(loss.item())\n",
        "        running_loss_val += loss.item()\n",
        "        #if (batch_i+1) % 20 == 0:\n",
        "        #    print(f\"    valid batch {i+1:3.0f} (of {len(dataloader):3.0f}) loss: {loss.item():.4f}\")\n",
        "        val_bar.set_postfix(loss=running_loss_val, epoch=epoch)\n",
        "        val_bar.update()\n",
        "\n",
        "    val_bar.set_postfix(loss=running_loss_val/dataset.get_num_batches(batch_size), epoch=epoch)\n",
        "    val_bar.update()\n",
        "\n",
        "    train_bar.n = 0\n",
        "    val_bar.n = 0\n",
        "    epoch_bar.update()\n",
        "\n",
        "    running_loss_val = running_loss_val / len(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jnixCTY1y2fR"
      },
      "outputs": [],
      "source": [
        "matplotlib.rc('figure', figsize=(15,4))\n",
        "val_ticks = [(i+1)*len(losses['train'])/len(losses['val']) for i in range(len(losses['val']))]\n",
        "plt.plot(range(len(losses['train'])), losses['train'], c='blue', lw=0.75)\n",
        "plt.plot(val_ticks, losses['val'], c='orange', lw=0.75)\n",
        "for i in range(num_epochs):\n",
        "    plt.axvline(x=i*len(losses['train'])/num_epochs, c='black', lw=0.25, alpha=0.5)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch and Batch')\n",
        "plt.legend(('Train','Validation'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMAycJtsy2fR"
      },
      "outputs": [],
      "source": [
        "# Test the model on full validation set\n",
        "dataset.set_split('val')\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_data in dataloader:\n",
        "        tweets = batch_data['x'].to(device)\n",
        "        labels = batch_data['y'].to(device)\n",
        "        outputs = model(tweets)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true += labels.tolist()\n",
        "        y_pred += predicted.tolist()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy (on {len(dataloader)*batch_size} validation tweets): {100 * correct / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71m36qGey2fS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=['positive','negative','neutral'])\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM_FbrcZy2fS"
      },
      "outputs": [],
      "source": [
        "import torchinfo\n",
        "torchinfo.summary(model, tuple(dataset[0]['x'].size()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9CYiDUsy2fS"
      },
      "source": [
        "Let's now retrieve the weight parameters that are associated with the words (i.e. tokens) that have similar meaning, \"great\", \"amazing\", \"incredible\". These words were in the vocabulary at following locations.\n",
        "* index for 'great': 4140\n",
        "* index for 'incredible': 4608\n",
        "* index for 'terrific': 7896"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HorsxW05y2fS"
      },
      "outputs": [],
      "source": [
        "fc1_weights = model.fc1.weight.data\n",
        "print(f\"first model layer has weight matrix with shape = {fc1_weights.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "465PxGROy2fS"
      },
      "outputs": [],
      "source": [
        "#wonderful_idx = vocab['wonderful']\n",
        "#incredible_idx = vocab['incredible']\n",
        "unit_i = 0\n",
        "print(f\"word1 index: {word1_idx}\")\n",
        "print(f\"  fc1_weights[{unit_i},{[word1_idx-1,word1_idx, word1_idx+1]}]: {fc1_weights[unit_i,word1_idx-1:word1_idx+2]}\")\n",
        "print(f\"word2 index: {word2_idx}\")\n",
        "print(f\"  fc1_weights[{unit_i},{[word2_idx-1,word2_idx, word2_idx+1]}]: {fc1_weights[unit_i,word2_idx-1:word2_idx+2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3obFdC-6y2fT"
      },
      "outputs": [],
      "source": [
        "diffs = {\"cont1\":[], \"word1_vs_word2\":[], \"cont2\":[]}\n",
        "for i in range(128):\n",
        "    diffs[\"cont1\"].append(abs(fc1_weights[i,word1_idx-1] - fc1_weights[i,word2_idx-1]))\n",
        "    diffs[\"word1_vs_word2\"].append(abs(fc1_weights[i,word1_idx] - fc1_weights[i,word2_idx]))\n",
        "    diffs[\"cont2\"].append(abs(fc1_weights[i,word1_idx+1] - fc1_weights[i,word2_idx+1]))\n",
        "\n",
        "# convert each list to a numpy array\n",
        "for key in diffs:\n",
        "    diffs[key] = np.array(diffs[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6JOPSFzy2fT"
      },
      "outputs": [],
      "source": [
        "# generate summary statistics for the differences for weight values\n",
        "diffs_df = pd.DataFrame(diffs)\n",
        "diffs_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W99wJwhiy2fT"
      },
      "outputs": [],
      "source": [
        "vocab = count_vectorizer.vocabulary_\n",
        "\n",
        "# find which key vocab is associated with the index 4139\n",
        "for key, value in vocab.items():\n",
        "    if value == word1_idx-1:\n",
        "        w_at_incredible_idx_minus_1 = key\n",
        "        print(f\"word at index {word1_idx-1}: {key}\")\n",
        "    if value == word2_idx-1:\n",
        "        w_at_wonderful_idx_minus_1 = key\n",
        "        print(f\"word at index {word2_idx-1}: {key}\")\n",
        "    if value == word1_idx+1:\n",
        "        w_at_incredible_idx_plus_1 = key\n",
        "        print(f\"word at index {word1_idx+1}: {key}\")\n",
        "    if value == word2_idx+1:\n",
        "        w_at_wonderful_idx_plus_1 = key\n",
        "        print(f\"word at index {word2_idx+1}: {key}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P4iyreYy2fT"
      },
      "outputs": [],
      "source": [
        "# plots of the differences as three different histograms\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(diffs[\"cont1\"], bins=20)\n",
        "# set x-axis limits to be the same for all three plots\n",
        "plt.xlim(0,0.4)\n",
        "plt.title(f\"cont1: {w_at_incredible_idx_minus_1} vs {w_at_wonderful_idx_minus_1}\")\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(diffs[\"word1_vs_word2\"], bins=20)\n",
        "plt.xlim(0,0.4)\n",
        "plt.title(f\"{word1} vs {word2}\")\n",
        "plt.subplot(1,3,3)\n",
        "plt.hist(diffs[\"cont2\"], bins=20)\n",
        "plt.xlim(0,0.4)\n",
        "plt.title(f\"cont2: {w_at_incredible_idx_plus_1} vs {w_at_wonderful_idx_plus_1}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmOWPMCRy2fT"
      },
      "outputs": [],
      "source": [
        "# length of an input is\n",
        "len(dataset[0]['x'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTl44wrvy2fT"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torchtext as text\n",
        "vec = text.vocab.GloVe(name='6B', dim=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x6s_s-4y2fU"
      },
      "source": [
        "---\n",
        "\n",
        "### Q2: Add your two words the list of words whose embeddings are displayed and compared. Do your two chosen words have similar embeddings? In other words, is the distance between your embeddings very small?\n",
        "\n",
        "Below you will choose your two words, which have similar meaning and which you suspect the model will treat similarly. Then, re-train the model and plot the histogram of the differences between learned weight values for each of your chosen words across the 128 hidden units in the first layer. Did the histograms show that the learned weight values were similar for your words? More similar than for the neighboring words compared to each other?\n",
        "\n",
        "I think it will be similar since they are used in the same way and show up constantly in all categories of tweets.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS6s_GQry2fU"
      },
      "outputs": [],
      "source": [
        "examples = ['annoy', 'annoyed', 'disappointed', 'sad', 'happy', 'pilot', 'attendant', 'crew', 'suitcase', 'luggage', 'carryon', 'great', 'amazing', 'terrific',\n",
        "'incredible', 'wonderful', 'flight','plane']\n",
        "embeddings = vec.get_vecs_by_tokens(examples, lower_case_backup=True)\n",
        "embeddings[0,:] # just the first embedding (you can verify by confirming that it is 50 elements long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT2kdfkXy2fU"
      },
      "outputs": [],
      "source": [
        "def compare_words_with_colors(vecs, wds):\n",
        "    wdsr = wds[:]\n",
        "    wdsr.reverse()\n",
        "\n",
        "    dim = len(vecs[0])\n",
        "\n",
        "    fig = plt.figure(num=None, figsize=(16, 4), dpi=80, facecolor='w', edgecolor='k')\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_facecolor('gray')\n",
        "\n",
        "    for i,v in enumerate(vecs):\n",
        "        ax.scatter(range(dim), [i]*dim, c=vecs[i], cmap='Spectral', s=150, marker='s')\n",
        "\n",
        "    plt.xticks(range(50), [i+1 for i in range(50)])\n",
        "    plt.xlabel('Dimension')\n",
        "    plt.yticks(range(len(wds)), wds)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "compare_words_with_colors(embeddings, examples)\n",
        "#examples.reverse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUIB7ZwQy2fU"
      },
      "outputs": [],
      "source": [
        "similarities = pairwise_cosine_similarity(embeddings, zero_diagonal=False)\n",
        "distances = 1 - similarities\n",
        "print(f\"the first row of the distance matrix for our set of words looks like: {distances[0,:]}\")\n",
        "pairwise_top = pd.DataFrame(\n",
        "    distances,\n",
        "    columns = examples,\n",
        "    index = examples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKiPsRARy2fU"
      },
      "source": [
        "In the cell above we created a distance matrix, let's now see what it looks like. Note that since we are plotting pairwise distances, larger values will be red and will suggest that the word the corresponding row is far away from the word in the corresponding columns (and vice versa).\n",
        "\n",
        "Similarly, words that are similar to each other will have a smaller distance (close to zero), and will be plotted in green."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv6orLyiy2fU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "#sns.color_palette(\"viridis\", as_cmap=True)\n",
        "sns.color_palette(\"mako\", as_cmap=True)\n",
        "sns.heatmap(\n",
        "    pairwise_top,\n",
        "    cmap='RdYlGn_r',  # Reverse the 'RdYlGn' colormap to have green for larger values and red for smaller values\n",
        "    linewidth=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LxBe-Ncy2fU"
      },
      "outputs": [],
      "source": [
        "data_URL = 'https://raw.githubusercontent.com/sgeinitz/DSML4220/main/data/airlinetweets.csv'\n",
        "df = pd.read_csv(data_URL)\n",
        "print(f\"df.shape: {df.shape}\")\n",
        "pd.set_option(\"display.max_colwidth\", 240)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocIICeBcy2fV"
      },
      "outputs": [],
      "source": [
        "random.seed(2)\n",
        "indices = list(range(len(df)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "df_test = df.iloc[indices[9000:],]\n",
        "df = df.iloc[indices[:9000],]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnKvzoOoy2fV"
      },
      "outputs": [],
      "source": [
        "df.sentiment.value_counts(normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pffRSLoy2fV"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\") # \"basic_english\"   \"subword\" uses revtok module (but does not work with GLoVE)\n",
        "df['tokens_raw'] = df['text'].apply(lambda x: tokenizer(x.lower()))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8cUzo1uy2fV"
      },
      "outputs": [],
      "source": [
        "df['tweet_length'] = df['tokens_raw'].apply(lambda x: len(x))\n",
        "#plt.figure(figsize=(12,6))\n",
        "#df['tweet_length'].hist() #bins=100, range=(0,45), width=0.9) #, df['tweet_length'].mean(), df['tweet_length'].median()\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gIZqGU_y2fV"
      },
      "outputs": [],
      "source": [
        "df.iloc[rows_w_word1,].index.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OnGBpcRy2fV"
      },
      "outputs": [],
      "source": [
        "tweet_i= 53\n",
        "tweet_embeddings = vec.get_vecs_by_tokens(df['tokens_raw'][tweet_i], lower_case_backup=True)\n",
        "print(f\"sentiment of this tweet: {df['sentiment'][tweet_i]}\")\n",
        "print(f\"tweet_embeddings.shape = {tweet_embeddings.shape}\")\n",
        "for i in range(df['tweet_length'][tweet_i]):\n",
        "    print(f\"    token, '{df['tokens_raw'][tweet_i][i]}' (at pos {i:2.0f}) has tweet_embeddings[:5] = {tweet_embeddings[i][:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3E3iGtMy2fV"
      },
      "outputs": [],
      "source": [
        "df.iloc[rows_w_word2,].index.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAed4TFVy2fW"
      },
      "outputs": [],
      "source": [
        "tweet_i= 18\n",
        "tweet_embeddings = vec.get_vecs_by_tokens(df['tokens_raw'][tweet_i], lower_case_backup=True)\n",
        "print(f\"sentiment of this tweet: {df['sentiment'][tweet_i]}\")\n",
        "print(f\"tweet_embeddings.shape = {tweet_embeddings.shape}\")\n",
        "for i in range(df['tweet_length'][tweet_i]):\n",
        "    print(f\"    token, '{df['tokens_raw'][tweet_i][i]}' (at pos {i:2.0f}) has tweet_embeddings[:5] = {tweet_embeddings[i][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts8saQYCy2fW"
      },
      "source": [
        "The tweet above had 9 tokens in it, which we can quickly confirm here by looking at the shape of it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_EuAAGWy2fW"
      },
      "outputs": [],
      "source": [
        "tweet_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwROa8YQy2fW"
      },
      "source": [
        "Before we continue we must decide what a good length will be for a max-length of the number of tokens to keep. Let's look at a histogram of the lenghts of each tweet (where length equals the number of raw tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCbQYk_yy2fW"
      },
      "outputs": [],
      "source": [
        "def meanTweetEmbeddings(raw_tokens):\n",
        "    embeddings = vec.get_vecs_by_tokens(raw_tokens, lower_case_backup=True)\n",
        "    n_embs = 0\n",
        "    emb_sum = torch.zeros((embeddings.shape[1]))\n",
        "    for i in range(min(embeddings.shape[0], 35)): # max number of tokens in a tweet is 35\n",
        "        if embeddings[i].abs().sum() > 0:\n",
        "            n_embs += 1\n",
        "            emb_sum += embeddings[i]\n",
        "    if n_embs > 0:\n",
        "        emb_avg = emb_sum / n_embs\n",
        "    else:\n",
        "        emb_avg = torch.zeros((embeddings.shape[1]))\n",
        "    if np.any(np.isnan(emb_avg.numpy())):\n",
        "        print(f\"exists an nan: {emb_sum}\")\n",
        "    return emb_avg\n",
        "\n",
        "X_int = df['tokens_raw'].apply(lambda x: meanTweetEmbeddings(x)).values\n",
        "print(f\"X_int.shape = {X_int.shape}\")\n",
        "X_int[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhiEt5jry2fW"
      },
      "outputs": [],
      "source": [
        "X_int[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhghTUAOy2fW"
      },
      "outputs": [],
      "source": [
        "if len(X_int[0] > 50):\n",
        "    avg_embedding = False\n",
        "else:\n",
        "    avg_embedding = True\n",
        "\n",
        "X = torch.stack(tuple(X_int))\n",
        "X.shape\n",
        "#X[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OknoU8fFy2fW"
      },
      "source": [
        "There should be 9000 rows in X, since this is the number of tweets (i.e. observations) in the training data.\n",
        "\n",
        "The number of columns is the _embedding size_ itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyZmBgxNy2fW"
      },
      "outputs": [],
      "source": [
        "labels = df['sentiment'].unique()\n",
        "enum_labels = enumerate(labels)\n",
        "label_to_idx = dict((lab, i) for i,lab in enum_labels)\n",
        "print(f\"label dictionary: {label_to_idx}\")\n",
        "y = torch.tensor([label_to_idx[lab] for lab in df['sentiment']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPCEWomsy2fX"
      },
      "outputs": [],
      "source": [
        "# Can be a good idea to occassionally check that the dims (or shapes) agree for the inputs (X) and labels (y)\n",
        "assert len(X) == len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yld1rICy2fX"
      },
      "outputs": [],
      "source": [
        "class AirlineTweetDataset(Dataset):\n",
        "    def __init__(self, observations, labels):\n",
        "        self.obs = observations\n",
        "        self.labs = labels\n",
        "        self.create_split(len(observations))\n",
        "\n",
        "    def create_split(self, n, seed=2, train_perc=0.7):\n",
        "        random.seed(seed)\n",
        "        indices = list(range(n))\n",
        "        random.shuffle(indices)\n",
        "        self._train_ids = list(indices[:int(n * train_perc)])\n",
        "        self._test_ids = list(indices[int(n * train_perc):])\n",
        "        self._split_X = self.obs[self._train_ids]\n",
        "        self._split_y = self.labs[self._train_ids]\n",
        "\n",
        "    def set_split(self, split='train'):\n",
        "        if split == 'train':\n",
        "            self._split_X = self.obs[self._train_ids]\n",
        "            self._split_y = self.labs[self._train_ids]\n",
        "        else:\n",
        "            self._split_X = self.obs[self._test_ids]\n",
        "            self._split_y = self.labs[self._test_ids]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._split_y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'x':self._split_X[idx], 'y':self._split_y[idx]}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "dataset = AirlineTweetDataset(X, y)\n",
        "dataset.create_split(len(X), seed=42, train_perc=0.85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrVovgbly2fX"
      },
      "outputs": [],
      "source": [
        "dataset.set_split('train')\n",
        "print(f\"len(dataset) = {len(dataset)}\")\n",
        "len(dataset[:]['x'])\n",
        "dataset[0]['x']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6s5lJXVy2fX"
      },
      "outputs": [],
      "source": [
        "assert not np.any(np.isnan(dataset[:]['x'].numpy()))\n",
        "assert np.all(np.isfinite(dataset[:]['x'].numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xHppd48y2fX"
      },
      "source": [
        "#### Hyperparameters for model with GloVe embeddings\n",
        "\n",
        "We'll use the same training configuration as before, although it is worth noting that this model would likely benefit from more training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5OK9n2Yy2fX"
      },
      "outputs": [],
      "source": [
        "# use same batch_size, learning_rate, and epochs as before\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GV3cgSFy2fX"
      },
      "outputs": [],
      "source": [
        "seed = 2\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# create dataset, model and define loss function and optimizer\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
        "model_w_embeddings = AirlineTweetClassifier(len(dataset[0]['x']), 128, 3)\n",
        "loss_fun = nn.CrossEntropyLoss()#weights)\n",
        "optimizer = torch.optim.Adam(model_w_embeddings.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeOjuehNy2fY"
      },
      "outputs": [],
      "source": [
        "epoch_bar = tqdm.notebook.tqdm(desc='training routine', total=num_epochs, position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm.notebook.tqdm(desc='split=train', total=dataset.get_num_batches(batch_size), position=1, leave=True)\n",
        "\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm.notebook.tqdm(desc='split=val', total=dataset.get_num_batches(batch_size), position=1, leave=True)\n",
        "\n",
        "losses = {'train':[], 'val':[]}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    model_w_embeddings.train()\n",
        "    running_loss_train = 0.0\n",
        "\n",
        "    for batch_i, batch_data in enumerate(dataloader):\n",
        "        tweets = batch_data['x'].to(device)\n",
        "        labels = batch_data['y'].to(device)\n",
        "\n",
        "        # forward\n",
        "        outputs = model_w_embeddings(tweets)\n",
        "        loss = loss_fun(outputs, labels)\n",
        "        losses['train'].append(loss.item())\n",
        "        running_loss_train += loss.item()\n",
        "\n",
        "        # backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #if (batch_i+1) % 10 == 0:\n",
        "        #    print(f\"    train batch {batch_i+1:3.0f} (of {len(dataloader):3.0f}) loss: {loss.item():.4f}\")\n",
        "            # update bar\n",
        "        train_bar.set_postfix(loss=running_loss_train, epoch=epoch)\n",
        "        train_bar.update()\n",
        "\n",
        "    train_bar.set_postfix(loss=running_loss_train/dataset.get_num_batches(batch_size), epoch=epoch)\n",
        "    train_bar.update()\n",
        "\n",
        "\n",
        "    running_loss_train = running_loss_train / len(dataset)\n",
        "\n",
        "    dataset.set_split('val')\n",
        "    model_w_embeddings.eval() # turn off the automatic differentiation\n",
        "    running_loss_val = 0.0\n",
        "\n",
        "    for batch_i, batch_data in enumerate(dataloader):\n",
        "        tweets = batch_data['x'].to(device)\n",
        "        labels = batch_data['y'].to(device)\n",
        "\n",
        "\n",
        "        # forward (no backward step for validation data)\n",
        "        outputs = model_w_embeddings(tweets)\n",
        "        loss = loss_fun(outputs, labels)\n",
        "        losses['val'].append(loss.item())\n",
        "        running_loss_val += loss.item()\n",
        "        #if (batch_i+1) % 20 == 0:\n",
        "        #    print(f\"    valid batch {i+1:3.0f} (of {len(dataloader):3.0f}) loss: {loss.item():.4f}\")\n",
        "        val_bar.set_postfix(loss=running_loss_val, epoch=epoch)\n",
        "        val_bar.update()\n",
        "\n",
        "    val_bar.set_postfix(loss=running_loss_val/dataset.get_num_batches(batch_size), epoch=epoch)\n",
        "    val_bar.update()\n",
        "\n",
        "    train_bar.n = 0\n",
        "    val_bar.n = 0\n",
        "    epoch_bar.update()\n",
        "\n",
        "    running_loss_val = running_loss_val / len(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt0Z_h7sy2fY"
      },
      "outputs": [],
      "source": [
        "matplotlib.rc('figure', figsize=(15,4))\n",
        "val_ticks = [(i+1)*len(losses['train'])/len(losses['val']) for i in range(len(losses['val']))]\n",
        "plt.plot(range(len(losses['train'])), losses['train'], c='blue', lw=0.75)\n",
        "plt.plot(val_ticks, losses['val'], c='orange', lw=0.75)\n",
        "for i in range(num_epochs):\n",
        "    plt.axvline(x=i*len(losses['train'])/num_epochs, c='black', lw=0.25, alpha=0.5)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch and Batch')\n",
        "plt.legend(('Train','Validation'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGJkvNc5y2fY"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "model_w_embeddings.eval()\n",
        "dataset.set_split('val')\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_data in dataloader:\n",
        "        tweets = batch_data['x'].to(device)\n",
        "        labels = batch_data['y'].to(device)\n",
        "        outputs = model_w_embeddings(tweets)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_true += labels.tolist()\n",
        "        y_pred += predicted.tolist()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy (on {len(dataloader)*batch_size} validation tweets): {100 * correct / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfREyShzy2fY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=['positive','negative','neutral'])\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DavJWcHfy2fY"
      },
      "outputs": [],
      "source": [
        "# length of an input is\n",
        "len(dataset[0]['x'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDOoCjbTy2fY"
      },
      "outputs": [],
      "source": [
        "import torchinfo\n",
        "torchinfo.summary(model_w_embeddings, tuple(dataset[0]['x'].size()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_bkTjFmy2fY"
      },
      "outputs": [],
      "source": [
        "50*128 + 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWgoaCqOy2fY"
      },
      "source": [
        "---\n",
        "\n",
        "### Q3: How much smaller is the model with embeddings than the model with one-hot encoded inputs?\n",
        "\n",
        "The model with embeddings will be significantly smaller because it isn't made up of a large number of vectors filled mostly with zeros and a few ones like one-hot encoding. While embedding contains fewer and more denser vectors.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU8WZr4Iy2fZ"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}