{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlassner/DSML_4220_Deep_Learning/blob/main/Lab5_lenet_w_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c98d84b4-5d86-4308-a0ef-117c0a52ddd0",
      "metadata": {
        "id": "c98d84b4-5d86-4308-a0ef-117c0a52ddd0"
      },
      "source": [
        "# Lab 5: LeNet from Scratch\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/DSML4220/blob/main/lab5_lenet_w_mnist.ipynb)\n",
        "\n",
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/DSML4220/blob/main/lab5_lenet_w_mnist.ipynb)\n",
        "\n",
        "In this notebook we'll build one of [LeNet CNNs](https://en.wikipedia.org/wiki/LeNet) from scratch. In addition to that, there are a few new Python modules/tools we'll use. These are:\n",
        "* [torchvision](https://pytorch.org/vision/stable/index.html) - a supplementary PyTorch module with popular image datasets, including [MNIST](https://en.wikipedia.org/wiki/MNIST_database)\n",
        "* [captum](https://captum.ai) - a tool that provides some utilities for model explainability and interpretability\n",
        "* [tensorboard](https://www.tensorflow.org/tensorboard/get_started) - developed for TensorFlow, TensorBoard provides model experimentation tools similar to Weights and Biases ([wandb.ai](https://wandb.ai)), and works with PyTorch too\n",
        "\n",
        "\n",
        "### Lab 5 Assignment/Task\n",
        "\n",
        "This lab has only three questions.  As before, you'll submit the link to your completed notebook with your answers to the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3300a8-82cd-4c4a-b3a0-81a3fb339758",
      "metadata": {
        "id": "8d3300a8-82cd-4c4a-b3a0-81a3fb339758"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sXK9A8feEbfK"
      },
      "id": "sXK9A8feEbfK"
    },
    {
      "cell_type": "markdown",
      "id": "b78f3dfa-1873-4ec2-afee-0aebdcfe78dc",
      "metadata": {
        "id": "b78f3dfa-1873-4ec2-afee-0aebdcfe78dc"
      },
      "source": [
        "## Dowload the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e278afe7-32d7-42a6-a21f-2a796b9314ba",
      "metadata": {
        "id": "e278afe7-32d7-42a6-a21f-2a796b9314ba"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64a74fb-313c-4792-87df-355861d00d5b",
      "metadata": {
        "id": "e64a74fb-313c-4792-87df-355861d00d5b"
      },
      "outputs": [],
      "source": [
        "train_val_dataset = datasets.MNIST(root=\"~/data/lenet/\", train=True, download=True)\n",
        "test_dataset = datasets.MNIST(root=\"~/data/lenet/\", train=False, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc06bfc4-c559-45ba-a8c7-0059e1a9f912",
      "metadata": {
        "id": "fc06bfc4-c559-45ba-a8c7-0059e1a9f912"
      },
      "source": [
        "## Analyze the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5f2d0d-9f00-42eb-a978-474582020a7f",
      "metadata": {
        "id": "0b5f2d0d-9f00-42eb-a978-474582020a7f"
      },
      "outputs": [],
      "source": [
        "train_val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd34c7e-0bfb-4cfa-b558-2b5819879a25",
      "metadata": {
        "id": "5dd34c7e-0bfb-4cfa-b558-2b5819879a25"
      },
      "outputs": [],
      "source": [
        "class_names = train_val_dataset.classes\n",
        "class_names[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd76aeb7-ba8d-4ffc-b3e0-b71b1c5aa160",
      "metadata": {
        "id": "cd76aeb7-ba8d-4ffc-b3e0-b71b1c5aa160"
      },
      "outputs": [],
      "source": [
        "class_index = train_val_dataset.class_to_idx\n",
        "class_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff71d2e-cd5f-47b7-9fc5-0ebd3fc51eed",
      "metadata": {
        "id": "3ff71d2e-cd5f-47b7-9fc5-0ebd3fc51eed"
      },
      "outputs": [],
      "source": [
        "img, label = train_val_dataset[0]\n",
        "img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8f5c2a-d69c-4596-87c3-0402270fa1da",
      "metadata": {
        "id": "4c8f5c2a-d69c-4596-87c3-0402270fa1da"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title(f\"{class_names[label]}\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f526067c-0815-4734-ab64-ef78091ccc25",
      "metadata": {
        "id": "f526067c-0815-4734-ab64-ef78091ccc25"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def img_pixel_superimpose(img, ax):\n",
        "    w, h = img.shape\n",
        "    color_map = plt.cm.get_cmap('gray_r')  # gray_reversed\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    for x in range(w):\n",
        "        for y in range(h):\n",
        "            color = color_map(img[x][y])\n",
        "            ax.annotate(str(img[x][y]), xy=(y,x), horizontalalignment='center', verticalalignment='center',\n",
        "                        color=color)\n",
        "            plt.axis(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f625b1-3510-48a4-9ab9-bcbe788e2cd9",
      "metadata": {
        "id": "e0f625b1-3510-48a4-9ab9-bcbe788e2cd9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "fig = plt.figure(figsize = (12,12))\n",
        "ax0 = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "rand_ind = torch.randint(0, len(train_val_dataset), size=[1]).item()\n",
        "\n",
        "img0 = train_val_dataset.data[rand_ind]\n",
        "img0 = img0.numpy()\n",
        "img_pixel_superimpose(img0, ax0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaece142-15f6-4aa3-9004-f31d14063ebd",
      "metadata": {
        "id": "aaece142-15f6-4aa3-9004-f31d14063ebd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(42)     # Search on the internet why '42' is special or\n",
        "                          # even '42 * 2' = 84\n",
        "\n",
        "fig = plt.figure(figsize=(16, 4))\n",
        "rows, cols = 2, 10\n",
        "\n",
        "for i in range(1, (rows*cols) + 1):\n",
        "    rand_ind = torch.randint(0, len(train_val_dataset), size=[1]).item()\n",
        "    img, lab = train_val_dataset[rand_ind]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f\"{class_names[lab]}\")\n",
        "    plt.axis(False)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516788a0-e824-4db9-bc89-3cb1babee060",
      "metadata": {
        "id": "516788a0-e824-4db9-bc89-3cb1babee060"
      },
      "source": [
        "## Transform the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85a6c48-ccaf-4977-b59e-f4df043ef295",
      "metadata": {
        "id": "d85a6c48-ccaf-4977-b59e-f4df043ef295"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13483007-66b5-4bc4-9cf3-7be1369906d4",
      "metadata": {
        "id": "13483007-66b5-4bc4-9cf3-7be1369906d4"
      },
      "source": [
        "### ToTensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8cccd29-1af1-4f4c-a63f-e5538b680e1a",
      "metadata": {
        "id": "b8cccd29-1af1-4f4c-a63f-e5538b680e1a"
      },
      "outputs": [],
      "source": [
        "train_val_dataset = datasets.MNIST(root=\"~/data/lenet/\", train=True, download=False, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root=\"~/data/lenet/\", train=False, download=False, transform=transforms.ToTensor())\n",
        "train_val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d971ea36-a0f9-4122-a315-2ced2a4f39d7",
      "metadata": {
        "tags": [],
        "id": "d971ea36-a0f9-4122-a315-2ced2a4f39d7"
      },
      "outputs": [],
      "source": [
        "img, lab = train_val_dataset[0]\n",
        "img, lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ab1ea8-0e55-4ba0-811e-6fe6210267a4",
      "metadata": {
        "id": "98ab1ea8-0e55-4ba0-811e-6fe6210267a4"
      },
      "outputs": [],
      "source": [
        "img.min(), img.max()  # The ToTensor() transformation scaled down 0-255 --> 0-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce68cad6-83e6-4a28-a086-bd65e6cc865d",
      "metadata": {
        "id": "ce68cad6-83e6-4a28-a086-bd65e6cc865d"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img.squeeze_(), cmap='gray')\n",
        "plt.title(f\"{class_names[lab]}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922ce250-adaf-4077-bd25-43b4f7d7ee94",
      "metadata": {
        "id": "922ce250-adaf-4077-bd25-43b4f7d7ee94"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83dfd6d2-2fc1-4bf8-80f2-5fcbcb719ef9",
      "metadata": {
        "id": "83dfd6d2-2fc1-4bf8-80f2-5fcbcb719ef9"
      },
      "outputs": [],
      "source": [
        "# Calculate mean and std\n",
        "\n",
        "imgs = torch.stack([img for img, _ in train_val_dataset], dim=0)\n",
        "print(imgs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74db4a5f-f5de-4ad5-a5c9-6e75afd07a86",
      "metadata": {
        "id": "74db4a5f-f5de-4ad5-a5c9-6e75afd07a86"
      },
      "outputs": [],
      "source": [
        "mean = imgs.view(1, -1).mean(dim=1)    # or imgs.mean()\n",
        "std = imgs.view(1, -1).std(dim=1)     # or imgs.std()\n",
        "mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae08948-58e8-4274-9acf-d319fc4e5e41",
      "metadata": {
        "id": "fae08948-58e8-4274-9acf-d319fc4e5e41"
      },
      "outputs": [],
      "source": [
        "mnist_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=mean, std=std)])\n",
        "mnist_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6faef05-1c62-4e9d-b696-07d3fb47c7b9",
      "metadata": {
        "id": "b6faef05-1c62-4e9d-b696-07d3fb47c7b9"
      },
      "outputs": [],
      "source": [
        "train_val_dataset = datasets.MNIST(root=\"~/data/lenet/\", train=True, download=False, transform=mnist_transforms)\n",
        "test_dataset = datasets.MNIST(root=\"~/data/lenet/\", train=False, download=False, transform=mnist_transforms)\n",
        "train_val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4171aac5-d12a-4bb9-868f-6b5c36b4a676",
      "metadata": {
        "tags": [],
        "id": "4171aac5-d12a-4bb9-868f-6b5c36b4a676"
      },
      "outputs": [],
      "source": [
        "img, label = train_val_dataset[0]\n",
        "img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6feee2-b484-4553-9651-de680f743d52",
      "metadata": {
        "id": "2e6feee2-b484-4553-9651-de680f743d52"
      },
      "outputs": [],
      "source": [
        "img.min(), img.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e40ea2c-d32a-4d84-ab7e-d221c2aa08b3",
      "metadata": {
        "id": "6e40ea2c-d32a-4d84-ab7e-d221c2aa08b3"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img.squeeze_(), cmap='gray')\n",
        "plt.title(f\"{class_names[label]}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a0930a9-a2d6-4458-8fa2-72791c57b34f",
      "metadata": {
        "id": "3a0930a9-a2d6-4458-8fa2-72791c57b34f"
      },
      "source": [
        "## Split dataset into Train/Val/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050f74f7-5a11-47a5-8e31-18a11d488b52",
      "metadata": {
        "id": "050f74f7-5a11-47a5-8e31-18a11d488b52"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.9 * len(train_val_dataset))\n",
        "val_size = len(train_val_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset=train_val_dataset, lengths=[train_size, val_size])\n",
        "train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902663f6-1c10-423e-911b-4479f516e913",
      "metadata": {
        "id": "902663f6-1c10-423e-911b-4479f516e913"
      },
      "outputs": [],
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18246f84-dce4-4754-9ca4-d7ffa66bd4a6",
      "metadata": {
        "id": "18246f84-dce4-4754-9ca4-d7ffa66bd4a6"
      },
      "outputs": [],
      "source": [
        "len(train_dataset), len(train_dataset.dataset)  # Remember train_dataset.dataset access parent train_val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acc36b0-d23a-4a38-93da-d90f9596b387",
      "metadata": {
        "id": "4acc36b0-d23a-4a38-93da-d90f9596b387"
      },
      "outputs": [],
      "source": [
        "# Validate train dataset is working fine\n",
        "img, label = train_dataset[0]\n",
        "plt.imshow(img.squeeze_(), cmap='gray')\n",
        "plt.title(f\"{class_names[label]}\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67478e3b-5338-4618-9dc7-2fb7250ea17b",
      "metadata": {
        "id": "67478e3b-5338-4618-9dc7-2fb7250ea17b"
      },
      "source": [
        "## Dataloader preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cbb8e15-11e4-4f3e-b9f6-2e085c98b075",
      "metadata": {
        "id": "5cbb8e15-11e4-4f3e-b9f6-2e085c98b075"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b799c3-e8ac-4407-85bd-277ed89444dd",
      "metadata": {
        "id": "10b799c3-e8ac-4407-85bd-277ed89444dd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583e9e13-b144-49c3-aeff-5de3d2225f63",
      "metadata": {
        "id": "583e9e13-b144-49c3-aeff-5de3d2225f63"
      },
      "outputs": [],
      "source": [
        "img, label = train_dataloader.dataset[0]\n",
        "img.shape, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d261a99a-6ab1-4aee-a122-34d85bec643a",
      "metadata": {
        "id": "d261a99a-6ab1-4aee-a122-34d85bec643a"
      },
      "outputs": [],
      "source": [
        "# Validate the dataloader is working fine\n",
        "plt.imshow(img.squeeze_(), cmap='gray')\n",
        "plt.title(f\"{class_names[label]}\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d99939d-d061-4ead-94ef-7cf0f6564496",
      "metadata": {
        "id": "4d99939d-d061-4ead-94ef-7cf0f6564496"
      },
      "outputs": [],
      "source": [
        "no_train_batches = len(train_dataloader.dataset) / train_dataloader.batch_size\n",
        "no_val_batches = len(val_dataloader.dataset) / val_dataloader.batch_size\n",
        "no_test_batches = len(test_dataloader.dataset) / test_dataloader.batch_size\n",
        "\n",
        "# Let's see no of batches that we have now with the current batch-size\n",
        "no_train_batches, no_val_batches, no_test_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19af41d-bcec-46c3-842c-f86a55656b65",
      "metadata": {
        "id": "b19af41d-bcec-46c3-842c-f86a55656b65"
      },
      "outputs": [],
      "source": [
        "len(train_dataloader), len(val_dataloader), len(test_dataloader)   # Actual lengths show wrapping at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452bed9d-f46b-49ab-9a04-bf016cda9f46",
      "metadata": {
        "id": "452bed9d-f46b-49ab-9a04-bf016cda9f46"
      },
      "source": [
        "# Model Architecture, Construct Training & Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f0c70c5-f51f-435a-9737-854fc4ac2967",
      "metadata": {
        "id": "7f0c70c5-f51f-435a-9737-854fc4ac2967"
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d07fb9-b662-4d23-8208-83131f5de569",
      "metadata": {
        "id": "94d07fb9-b662-4d23-8208-83131f5de569"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            #1\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),   # 28*28->32*32-->28*28\n",
        "            nn.ReLU(),  # nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  # 14*14\n",
        "\n",
        "            #2\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),  # 10*10\n",
        "            nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),  # 5*5\n",
        "\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=16*5*5, out_features=120),\n",
        "            nn.BatchNorm1d(120), # not part of original LeNet\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(in_features=84, out_features=10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.feature(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ffe3f01-4d61-4bee-b3d2-383e847bea37",
      "metadata": {
        "id": "1ffe3f01-4d61-4bee-b3d2-383e847bea37"
      },
      "outputs": [],
      "source": [
        "lenetmodel = LeNet()\n",
        "lenetmodel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb80ff8-becd-4c9c-acd3-a96c716fd04d",
      "metadata": {
        "id": "9bb80ff8-becd-4c9c-acd3-a96c716fd04d"
      },
      "source": [
        "### Model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abb25a1-e12c-4c79-af16-8899ccd399ac",
      "metadata": {
        "id": "7abb25a1-e12c-4c79-af16-8899ccd399ac"
      },
      "outputs": [],
      "source": [
        " !pip install torchinfo\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368da325-287b-4ae1-940a-a698f932585f",
      "metadata": {
        "id": "368da325-287b-4ae1-940a-a698f932585f"
      },
      "outputs": [],
      "source": [
        "summary(model=lenetmodel, input_size=(1, 1, 28, 28), col_width=20, col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ca4087-8ab6-479d-ad96-43b549ebd7ad",
      "metadata": {
        "id": "78ca4087-8ab6-479d-ad96-43b549ebd7ad"
      },
      "source": [
        "### Loss, Optimizer, Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b506ebbe-e128-449e-8137-0235f6238023",
      "metadata": {
        "id": "b506ebbe-e128-449e-8137-0235f6238023"
      },
      "outputs": [],
      "source": [
        " !pip install torchmetrics\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=lenetmodel.parameters(), lr=0.001)\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "accuracy = Accuracy(task='multiclass', num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9db8539-0512-47d5-b20e-0767b6c62c43",
      "metadata": {
        "id": "d9db8539-0512-47d5-b20e-0767b6c62c43"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c17f350-afdd-4015-879c-1083a9146509",
      "metadata": {
        "id": "7c17f350-afdd-4015-879c-1083a9146509"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Experiment tracking\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "experiment_name = \"MNIST\"\n",
        "model_name = \"LeNet\"\n",
        "log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "# device-agnostic setup\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "accuracy = accuracy.to(device)\n",
        "lenetmodel = lenetmodel.to(device)\n",
        "\n",
        "EPOCHS = 12\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    # Training loop\n",
        "    train_loss, train_acc = 0.0, 0.0\n",
        "    for X, y in train_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        lenetmodel.train()\n",
        "\n",
        "        y_pred = lenetmodel(X)\n",
        "\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        acc = accuracy(y_pred, y)\n",
        "        train_acc += acc\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_acc /= len(train_dataloader)\n",
        "\n",
        "    # Validation loop\n",
        "    val_loss, val_acc = 0.0, 0.0\n",
        "    lenetmodel.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in val_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            y_pred = lenetmodel(X)\n",
        "\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            acc = accuracy(y_pred, y)\n",
        "            val_acc += acc\n",
        "\n",
        "        val_loss /= len(val_dataloader)\n",
        "        val_acc /= len(val_dataloader)\n",
        "\n",
        "    writer.add_scalars(main_tag=\"Loss\", tag_scalar_dict={\"train/loss\": train_loss, \"val/loss\": val_loss}, global_step=epoch)\n",
        "    writer.add_scalars(main_tag=\"Accuracy\", tag_scalar_dict={\"train/acc\": train_acc, \"val/acc\": val_acc}, global_step=epoch)\n",
        "\n",
        "    print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9646ff5b-368b-4389-be5f-ab006551600e",
      "metadata": {
        "id": "9646ff5b-368b-4389-be5f-ab006551600e"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b3070f-25cc-424f-8756-2344e3889493",
      "metadata": {
        "id": "01b3070f-25cc-424f-8756-2344e3889493"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "MODEL_PATH = Path(\"~/models/lenet/\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"lenet.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2deab4-2fa5-4e62-a907-25bb86c97bbc",
      "metadata": {
        "id": "bc2deab4-2fa5-4e62-a907-25bb86c97bbc"
      },
      "outputs": [],
      "source": [
        "print(f\"Saving the model: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=lenetmodel.state_dict(), f=MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a599a7f-36f6-498b-b42f-08278aa6f0fe",
      "metadata": {
        "id": "0a599a7f-36f6-498b-b42f-08278aa6f0fe"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9188e563-772e-441d-824f-6d055a0ec18c",
      "metadata": {
        "id": "9188e563-772e-441d-824f-6d055a0ec18c"
      },
      "outputs": [],
      "source": [
        "lenetmodel_loaded = LeNet()\n",
        "lenetmodel_loaded.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "lenetmodel_loaded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a3451b-010c-43b9-a507-c588ff1d2781",
      "metadata": {
        "id": "91a3451b-010c-43b9-a507-c588ff1d2781"
      },
      "source": [
        "### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc415e6d-41ef-4649-b191-749824a1363e",
      "metadata": {
        "id": "fc415e6d-41ef-4649-b191-749824a1363e"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = 0, 0\n",
        "\n",
        "lenetmodel_loaded.to(device)\n",
        "\n",
        "lenetmodel_loaded.eval()\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = lenetmodel_loaded(X)\n",
        "\n",
        "        test_loss += loss_fn(y_pred, y)\n",
        "        test_acc += accuracy(y_pred, y)\n",
        "\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "print(f\"Test loss: {test_loss: .5f}| Test acc: {test_acc: .5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e497545-a6dd-4fbb-a525-c6089d0e8865",
      "metadata": {
        "id": "9e497545-a6dd-4fbb-a525-c6089d0e8865"
      },
      "outputs": [],
      "source": [
        "# See random images with their labels\n",
        "torch.manual_seed(42)  # setting random seed\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "rows, cols = 2, 6\n",
        "for i in range(1, (rows * cols) + 1):\n",
        "    random_idx = torch.randint(0, len(test_dataset), size=[1]).item()\n",
        "    img, label_gt = test_dataset[random_idx]\n",
        "    img_temp = img.unsqueeze(dim=0).to(device)\n",
        "    # print(img.shape)\n",
        "    label_pred = torch.argmax(lenetmodel_loaded(img_temp))\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    img = img.permute(1, 2, 0)    # CWH --> WHC\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    if label_pred == label_gt:\n",
        "        plt.title(class_names[label_pred], color='g')\n",
        "    else:\n",
        "        plt.title(class_names[label_pred], color='r')\n",
        "    plt.axis(False)\n",
        "    plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e38c84a7-84a5-47b2-a662-9517f37f73c9",
      "metadata": {
        "id": "e38c84a7-84a5-47b2-a662-9517f37f73c9"
      },
      "source": [
        "### Model Interpretability/Explainability\n",
        "\n",
        "Let's now use `Captum` to try to understand more about why the model makes the predictions that it does. Specifically, we want to try to understand what it is about one observation that causes the model to make the prediction that it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce94add6-dbb2-48eb-b628-97ed38a38922",
      "metadata": {
        "id": "ce94add6-dbb2-48eb-b628-97ed38a38922"
      },
      "outputs": [],
      "source": [
        "!pip install captum\n",
        "from captum.attr import IntegratedGradients, Occlusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b33e77",
      "metadata": {
        "id": "e2b33e77"
      },
      "source": [
        "This first approach uses the parameter gradients but calculated for one specific observation. This may sound odd, particularly since we are not training, but calculating the gradient will give us idea of how the 'surface' for this one observation differs from the average 'surface' that the model was trained to find a minimum for. This will give us an idea of which features of this image (which pixels) make it most unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c48c2852-0b2e-4e9b-a8f6-d184338c0480",
      "metadata": {
        "id": "c48c2852-0b2e-4e9b-a8f6-d184338c0480"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(109)#108)\n",
        "\n",
        "random_idx = torch.randint(0, len(test_dataset), size=[1]).item()\n",
        "img, label = test_dataset[random_idx]\n",
        "img = img.to(device)\n",
        "\n",
        "# Instantiate an IntegratedGradients object for the model\n",
        "ig = IntegratedGradients(lenetmodel_loaded)\n",
        "\n",
        "# Compute the attribution scores for the random image\n",
        "attr, delta = ig.attribute(img.unsqueeze(0), target=label, return_convergence_delta=True)\n",
        "\n",
        "# Visualize the attribution scores\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].imshow(img.permute(1, 2, 0).to('cpu'), cmap='gray')\n",
        "ax[1].imshow(attr[0][0].detach().to('cpu').numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f23543",
      "metadata": {
        "id": "31f23543"
      },
      "source": [
        "Next, let's try a different tool that is based on hiding part of the input and seeing how much prediction differs. Similar to a convolutional kernel, this `Feature Occlusion`, of _feature hiding_ approach will use a small sliding window over the input. When an area is hidden, and then yields a much different prediction than it would if it were not hidden, are considered more influential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d91b34",
      "metadata": {
        "id": "c4d91b34"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(109)\n",
        "\n",
        "random_idx = torch.randint(0, len(test_dataset), size=[1]).item()\n",
        "img, label = test_dataset[random_idx]\n",
        "img = img.to(device)\n",
        "\n",
        "# Instantiate an IntegratedGradients object for the model\n",
        "oc = Occlusion(lenetmodel_loaded)\n",
        "\n",
        "# Compute the attribution scores for the random image\n",
        "attr = oc.attribute(img.unsqueeze(0), target=label, strides=(1, 3, 1), sliding_window_shapes=(1, 3, 3), baselines=0)\n",
        "\n",
        "# Visualize the attribution scores\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].imshow(img.permute(1, 2, 0).to('cpu'), cmap='gray')\n",
        "ax[1].imshow(attr[0][0].detach().to('cpu').numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb7bea0",
      "metadata": {
        "id": "0cb7bea0"
      },
      "source": [
        "---\n",
        "\n",
        "Q1: The stride parameter is a 3-tuple with values for channel, horizontal, and vertical shifts. We want to leave the channel parameter at 1 since the inputs are only 1-channel, but...\n",
        "\n",
        "* Q1a: Try modifying the horizontal stride parameter though (e.g. `stride=(1,3,1)`), how does the output image differ?\n",
        "\n",
        "The image is slightly sharper with the stride being (1,3,1), but overall they are still very similar.\n",
        "\n",
        "* Q1b: Now change the horizontal parameter back to 1 and try changing the vertical stride parameter so that `stride=(1,1,3)`. Now how does the output immage differ?\n",
        "\n",
        "It is slightly more clearer compared to (1,1,1) and (1,3,1). The shape of the four is definitly more identifiable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ef5b97-ce25-4ec4-8515-2973a85c3ab2",
      "metadata": {
        "id": "34ef5b97-ce25-4ec4-8515-2973a85c3ab2"
      },
      "source": [
        "### Visualize model metrics\n",
        "\n",
        "Similar to Weights and Biases, the module TensorBoard is a machine learning experimentation tool for tracking and visualizing metrics from training runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a0c71a-e405-4848-a3ee-24e58e017249",
      "metadata": {
        "id": "90a0c71a-e405-4848-a3ee-24e58e017249"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cf1876-cf88-4a97-a3ac-73c068919b7c",
      "metadata": {
        "id": "12cf1876-cf88-4a97-a3ac-73c068919b7c"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"runs/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6cd0fb9",
      "metadata": {
        "id": "c6cd0fb9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4c6fdffc",
      "metadata": {
        "id": "4c6fdffc"
      },
      "source": [
        "---\n",
        "\n",
        "#### Q2: Change the sigmoid activation functions to use ReLU instead. How does this change the results? Specifically, what the Validation accuracy when using Sigmoid? Then, what was it when using ReLU instead of Sigmoid?\n",
        "\n",
        "The validation accuracy for sigmoid is 0.98172 and for ReLU it was 0.98321 which was slightly higher.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01d08fb",
      "metadata": {
        "id": "c01d08fb"
      },
      "source": [
        "---\n",
        "\n",
        "#### Q3: Add Batch normalization to the first linear layer in the model.\n",
        "\n",
        "* Q3a: Does using Batch Normalization improve the performance (accuracy) of the model? By how much? (_recall that batch normalization was not discovered/used until around 2015, so that is why it was not used in LeNet_)\n",
        "\n",
        "It did increase the accuracy by 0.004 with a 0.98753 score.\n",
        "\n",
        "* Q3b: Does using Batch Normalization increase the number of parameters in the model? If so, how many more parameters are there? Can you explain why?\n",
        "\n",
        "The number of parameters for batch normalization was 61,946. Without BN, there are  61,706. BN adds two new parameters: stretch and shift, which allows it more flexibility to learn better. This causes a slight increase in the number of parameters.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}