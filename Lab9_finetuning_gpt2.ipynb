{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlassner/DSML_4220_Deep_Learning/blob/main/Lab9_finetuning_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMZVm4kH-AF"
      },
      "source": [
        "# Lab 9: Finetuning GPT-2 with LoRA\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/DSML4220/blob/main/lab9_finetuning_gpt2.ipynb)\n",
        "\n",
        "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/DSML4220/blob/main/lab9_finetuning_gpt2.ipynb)\n",
        "\n",
        "In this lab we will use GPT-2 for the task of text generation. We'll first quickly compare Greedy Search and (Diverse) Beam Search with GPT-2. Then we'll finetune GPT-2 to generate text that is more explicitly infused with knowledge of Hemingway's book, \"_The Sun also Rises_\", and can generate text in the style of the book.\n",
        "\n",
        "\n",
        "### Lab 9 Assignment/Task\n",
        "There are three questions in this lab. As an added bonus, try downloading your own book from Project Gutenberg to finetune GPT-2 to generate text following your chosen book/author (see [this script](https://github.com/sgeinitz/DSML4220/blob/main/convert_book2csv.py)) for help to convert it to a .csv file of sentences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKCJxsS6hRNy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import GPT2Tokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from peft import LoraModel, LoraConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WMF0IBZhYA_"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEHusW_JhmgZ"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYvuHZU-4Kf"
      },
      "source": [
        "Let's generate some text from the model using regular Greedy Search (here is the [HuggingFace example documenting this](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores.example))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rqVbKv3rvfv"
      },
      "outputs": [],
      "source": [
        "# Example 1: Print the scores for each token generated with Greedy Search\n",
        "#tokenizer.pad_token_id = tokenizer.eso_token_id\n",
        "outputs = model.generate(**inputs, max_new_tokens=10, return_dict_in_generate=True, output_scores=True)\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, normalize_logits=True\n",
        ")\n",
        "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
        "# encoder-decoder models, like BART or T5.\n",
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    # | token | token string | log probability | probability\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mQQW3GCkBbu"
      },
      "outputs": [],
      "source": [
        "outputs['sequences']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGURJAW1_NyV"
      },
      "source": [
        "Let's now use Beam Search (again using [this example from HF](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores.example))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuzkpfBkhdCj"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "\n",
        "# Approach 2: Beam Search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=10,\n",
        "    num_beams=6,\n",
        "    num_beam_groups=3,\n",
        "    diversity_penalty=5.0,\n",
        "    num_return_sequences=6,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        ")\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU90HrOijFmu"
      },
      "outputs": [],
      "source": [
        "outputs['sequences']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOQUIhTxit1I"
      },
      "outputs": [],
      "source": [
        "for s, seq in enumerate(outputs['sequences']):\n",
        "  print(f\"seq {s}: {tokenizer.decode(seq)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW7N-MviY5gb"
      },
      "source": [
        "---\n",
        "\n",
        "### Q1: Does the Beam Search above use Diverse Beam Search? If not, change it to use Diverse Beam Search and describe how the output differs.  \n",
        "\n",
        "(Hint: Look a few cells down at the next use of Beam Search, there are two parameters you will need to add, `num_beam_groups`, and `diversity_penalty`)\n",
        "\n",
        "The outputs differ vastly when num_beams_groups and diversity_penatly are added. Without them, the sentences had very few differences-the last few words. However, those added variables added 3 separate scenarios of possible words after \"Today is\".\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXgB8_qorxvL"
      },
      "outputs": [],
      "source": [
        "prompt = [\"Cohn confronted the bullfighter and \"]\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "max_new_toks = 15\n",
        "# Example 1: Print the scores for each token generated with Greedy Search\n",
        "#outputs = model.generate(**inputs, max_new_tokens=max_new_toks, return_dict_in_generate=True, output_scores=True, do_sample=True, temperature=1)\n",
        "outputs = model.generate(**inputs, max_new_tokens=max_new_toks, return_dict_in_generate=True, output_scores=True, do_sample=False)\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, normalize_logits=True\n",
        ")\n",
        "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
        "# encoder-decoder models, like BART or T5.\n",
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    # | token | token string | log probability | probability\n",
        "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMa12IGRsVUS"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Approach 2: Reconstruct the sequence scores from Beam Search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=max_new_toks,\n",
        "    num_beams=6,\n",
        "    num_beam_groups=3,\n",
        "    diversity_penalty=5.0,\n",
        "    num_return_sequences=6,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    temperature=1.0,\n",
        "    #do_sample=True\n",
        ")\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")\n",
        "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
        "# Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
        "# use case, you might want to recompute it with `normalize_logits=True`.\n",
        "# Tip 2: the output length does NOT include the input length\n",
        "output_length = np.sum(transition_scores.numpy() < 0, axis=1)\n",
        "length_penalty = model.generation_config.length_penalty\n",
        "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
        "\n",
        "print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n",
        "\n",
        "for s, seq in enumerate(outputs['sequences']):\n",
        "  print(f\"seq {s}: {tokenizer.decode(seq)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQFiVKZjYVZd"
      },
      "source": [
        "Let's now load the raw text from Hemingway's book, _\"The Sun also Rises\"_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc-mYE4724pw"
      },
      "outputs": [],
      "source": [
        "heming = pd.read_csv(\"https://raw.githubusercontent.com/sgeinitz/DSML4220/main/data/sunalsorises.csv\")\n",
        "heming.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLalprGWvDVf"
      },
      "outputs": [],
      "source": [
        "sentences = heming['sentence']\n",
        "sentences.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Tf92NAvUQn"
      },
      "outputs": [],
      "source": [
        "print(f\"        sentence: '{sentences[0]}' \\n is tokenized as: {tokenizer.encode(sentences[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlyiUDBpvMgl"
      },
      "outputs": [],
      "source": [
        "max_length = max([len(tokenizer.encode(sentence)) for sentence in sentences])\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M_fiUgnv-4-"
      },
      "outputs": [],
      "source": [
        "class HemingwayDataset(Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, max_length):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for txt in txt_list:\n",
        "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
        "                                       max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self): # overload the len() Python built-in function\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx): # overload the [] operator\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cDS45JawDMI"
      },
      "outputs": [],
      "source": [
        "dataset = HemingwayDataset(sentences, tokenizer, max_length=max_length)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEe3658IwLtC"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw_TgDI973Sj"
      },
      "source": [
        "Notice that above we set the `pad_token_id` to be the same as the `eos_token_id` (i.e. end-of-stream token id). So all of those `50256` entries above are being used as end-of-stream, or end-of-sequence tokens (except the first one, which is denoting the end of the sequence).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9JQMBHkwVqs"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode([50256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkgK6X_gwb5M"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "n_epochs = 2\n",
        "training_args = TrainingArguments(output_dir='~/hemingway_generation', num_train_epochs=n_epochs, logging_steps=100, save_steps=500, do_eval=True,\n",
        "                                  eval_steps=20, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, save_safetensors=False,\n",
        "                                  warmup_steps=10, weight_decay=0.05, logging_dir='~/hemingway_generation/logs', report_to='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2UZgnRJGKd7"
      },
      "source": [
        "Let's load GPT-2 and then  take a rough glance at the architecture of GPT (w/ ~130M parameters) by printing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcM5NuAVXMFY"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KezOmgkkGUMM"
      },
      "outputs": [],
      "source": [
        "def count_trainable_parameters(mod):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, mod.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params\n",
        "\n",
        "gpt2_params = count_trainable_parameters(model)\n",
        "print(f\"GPT-2 trainable parameters: {gpt2_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ92jn8qwzbV"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model,  args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "                  data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                              'labels': torch.stack([f[0] for f in data])})\n",
        "# on Colab this will take 6+hrs w/ cpu or <10min w/ T4 GPU per epoch\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WAS8IqWa1Iq"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Use Diverse Beam Search\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=max_new_toks,\n",
        "    num_beams=6,\n",
        "    num_beam_groups=3,\n",
        "    diversity_penalty=5.0,\n",
        "    num_return_sequences=5,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    temperature=2.0,\n",
        "    #do_sample=True\n",
        ")\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")\n",
        "\n",
        "output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
        "length_penalty = model.generation_config.length_penalty\n",
        "reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**length_penalty)\n",
        "\n",
        "for s, seq in enumerate(outputs['sequences']):\n",
        "  gen_text = tokenizer.decode(seq)\n",
        "  # remove everything from '<|endoftext|>' on at the end of gen_text\n",
        "  gen_text = gen_text[:gen_text.find('<|endoftext|>')]\n",
        "  print(f\"seq {s}: {gen_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBYHnqkhNuNY"
      },
      "source": [
        "Next, let's use LoRA to fine tune the model. We'll load the model again to ensure that the earlier finetuning is not included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grzQdZ0FCJFy"
      },
      "outputs": [],
      "source": [
        "# load the model again so that we can use LoRA\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQVY2kEGw8Ir"
      },
      "outputs": [],
      "source": [
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\", \"wte\", \"c_fc\", \"c_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.01,\n",
        "    fan_in_fan_out=True\n",
        ")\n",
        "\n",
        "lora_model = LoraModel(model, lora_config, \"default\")\n",
        "lora_model.model.tie_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFmBFdUcDmK8"
      },
      "outputs": [],
      "source": [
        "print(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0i5g63JC4m9"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=lora_model,  args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "                  data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                              'labels': torch.stack([f[0] for f in data])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqzHwXzgDBsy"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd8ug9Vii9dy"
      },
      "source": [
        "---\n",
        "\n",
        "### Q2: How many more `training_samples_per_second` could the LoRA model get through during finetuning than the original GPT-2 model could?\n",
        "\n",
        "The LoRA was able to get through 2 more training_samples_per_second than the GPT-2 model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byXjmmgsVgEE"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Use Diverse Beam Search\n",
        "outputs = lora_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=max_new_toks,\n",
        "    num_beams=6,\n",
        "    num_beam_groups=3,\n",
        "    diversity_penalty=5.0,\n",
        "    num_return_sequences=5,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    temperature=1.5,\n",
        "    #do_sample=True\n",
        ")\n",
        "transition_scores = lora_model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
        ")\n",
        "\n",
        "output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
        "length_penalty = lora_model.generation_config.length_penalty\n",
        "reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**length_penalty)\n",
        "\n",
        "for s, seq in enumerate(outputs['sequences']):\n",
        "  gen_text = tokenizer.decode(seq)\n",
        "  # remove everything from '<|endoftext|>' to the end from gen_text\n",
        "  gen_text = gen_text[:gen_text.find('<|endoftext|>')]\n",
        "  print(f\"seq {s}: {gen_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM3FTn-fCDGG"
      },
      "outputs": [],
      "source": [
        "lora_params = count_trainable_parameters(lora_model)\n",
        "print(f\"LoRA trainable parameters: {lora_params} ({(100*lora_params/gpt2_params):.2f}% of GPT-2's trainable parameters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3UxIzbCjOXz"
      },
      "source": [
        "---\n",
        "\n",
        "### Q3: How many fewer parameters did the LoRA model need to train/tune than the full GPT-2 model did?\n",
        "\n",
        "(Hint: See output from above cell)\n",
        "\n",
        "It used 2.08% of GPT-2's trainable parameters.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}